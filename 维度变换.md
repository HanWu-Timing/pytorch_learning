# 维度变换

### view/reshape：完全一致, pytorch 0.4之后reshape自带contiguous

-   操作后size()须保持不变
-   b=a.view()之后，a的结构信息丢失，无法恢复

### squeeze/unsqueeze: 压缩/展开

#### unsqueeze(index) 在索引之前插入，取值范围是\[-a.dim()-1, a.dim()+1)

如：

```python
a.shape   torch.Size([4,1,28,28])
a.unsqueeze(0).shape  torch.Size([1, 4, 1, 28, 28])
a.unsqueeze(-5).shape  torch.Size([1, 4, 1, 28, 28])
a= torch.tensor([1.2,2.3])
a.unsqueeze(-1)  tensor([[1.2000],
                        [2.3000]])
a.unsqueeze(0)
tensor([[1.2000, 2.3000]])

```

#### squeeze(index)：只能挤压大小为1的维度

```python
b.shape   torch.Size([1, 32, 1, 1])
b.squeeze().shape  torch.Size([32]) # 能压缩的全部压缩
b.squeeze(0).shape torch.Size([32, 1, 1])


```

### expand/repeat 改变维度大小

#### expand: 通过广播扩展，改变数据的理解方式，而不改变数据（执行速度快，节约内存），只限定从1扩展到N的操作

```python
b.shape  torch.Size([1, 32, 1, 1])
b.expand(4, 32, 14, 14).shape  torch.Size([4, 32, 14, 14])
b.expand(-1, 32, -1, -4).shape  torch.Size([1, 32, 1, -4]) # -1代表不变，-4无意义
```

#### repeat：通过内存拷贝，会改变数据本身，参数代表拷贝次数。使用可能造成内存空间的浪费

```python
b.shape  torch.Size([1, 32, 1, 1])
b.repeat(4, 32, 1, 1).shape  torch.Size([4, 1024, 1, 1])

```

### t 转置

只适用于2D的矩阵

```python
b.t()  # 报错
a=torch.randn(3, 4)  a.t()  # 通过，变成了4行3列的
```

### Transpose 维度交换

```python
a.shape  # [4， 3， 32， 32]
a1 = a.transpose(1, 3).view(4, 3*32*32).view（4, 3, 32, 32)  
# 报错 维度信息丢失且transpose之后数据维度被打乱
# 打乱指虽然改变了张量的形状，但是张量在内存中的存储顺序仍然不变
a1 = a.transpose(1, 3).contiguous().view(4, 3*32*32).view（4, 3, 32, 32)
# contiguous()把数据连在一起，即先拷贝了一份张量在内存中的地址，
# 然后将地址按照形状改变后的张量的语义进行排列。不报错，但实际是错的
a2 = a.transpose(1, 3).contiguous().view(4, 3*32*32).view（4, 3, 32, 32).tranpose(1,3)
# a2 和 a 完全一致
```

### Permute 维度变换

```python
b = torch.rand(4, 3, 28, 32) # b c h w
b.transpose(1, 3).tanspose(1, 2).shape  torch.Size([4, 28, 32, 3])  # b h w c
# 经过两次transpose才达到
b.permute(0, 2, 3, 1).shape  torch.Size([4, 28, 32, 3])  # 一步到位  

```
